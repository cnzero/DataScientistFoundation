{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言\n",
    "并不完全按照教材来的。主要是想在教材多遍阅读与理解的基础上，更要有自己的理解与整理方式。\n",
    "\n",
    "并且强调问题引导方式进行 __自问自答__ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是卷积运算与池化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<video width=\"250\" height=\"180\" src=\"fgConvolution.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<img src=\"Convolution_schematic.gif\" style=\"width:300px;height:200px;\">\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<video width=\"300\" height=\"200\" src=\"conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "__卷积运算__ 在数学上有严格的公式定义，但是由于在CNN中常常不翻折，因此也称为cross-convolution。 \n",
    "\n",
    "理解成一种计算重叠部分的运算，相乘或element-wise相乘求和等。\n",
    "\n",
    "如上所示为一维和二维卷积操作，用图示或动画比较直观，就不打算上公式了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<td>\n",
    "<img src=\"max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积运算的Motivations\n",
    "- sparse intersection\n",
    "- parameter sharing\n",
    "- equivariant to translation\n",
    "\n",
    "这是CNN网络的三个精髓，从这三点出发，从本质上理解何为CNN已经能较好地处理哪些问题，并在不同情况下进行一定的配置。\n",
    "\n",
    "但三者并不是独立的，而是相互关联，也就是从三个方面阐述了CNN的特点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparse intersections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__如何理解这里的稀疏?__\n",
    "\n",
    "前一层_仅部分_节点上的元素对后一层的有贡献，而当网络层次加深时，能够 _间接_ 地从前一层或更前层网络节点元素获得贡献。\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"fig92.JPG\" style=\"width:400px;height:240px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"fig93.JPG\" style=\"width:400px;height:240px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"fig94.JPG\" style=\"width:400px;height:240px;\">\n",
    "<td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN与FNN的对比__\n",
    "- Left 从下往上看 \n",
    "    1. CNN 中 $x_3$ 仅对 $a_2, a_3, a_4$有贡献；\n",
    "    2. FNN 中 $x_3$ 对下一层所有节点都有贡献\n",
    "- Center 从上往下看\n",
    "    1. CNN 中 $a_3$ 仅来自于 $x_2, x_3, x_4$的贡献\n",
    "    2. FNN 中 $a_3$ 对来自上一层所节点的贡献 \n",
    "- Right\n",
    "    当CNN网络层次加深时，能够看到节点间接地受更宽更深节点的影响。\n",
    "    \n",
    "__小结__：\n",
    "\n",
    "CNN初步假设，相邻两层间仅局部关联或有贡献，但是这个假设可能在某些情况下过于强烈约束或不适用，此时可以通过加深网络层次放缩约束；同时加深层次能够经过更多的non-linear activation functions 也就是对原数据进行更多非线性变换或抽象。\n",
    "\n",
    "另外，由于局部关联或贡献，极大地减少了运算量和对权重矩阵的存储。\n",
    "1. computational efficiency\n",
    "2. less memory requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<img src=\"fig95.JPG\" style=\"width:500px;height:300px;\">\n",
    "<td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在CNN中仅仅存在权重系数 $a, b, c$ 并在多处使用，这也就是称为 __parameters sharing__\n",
    "2. 在FNN中，任何一个权重系数 $k_{ij}$ 仅仅被使用一次，也就意味着需要通过更多的有标签样本，进行迭代训练，得到最优化的权重矩阵。\n",
    "\n",
    "同样，CNN的 _parameter sharing_ 实际上是在FNN的基础上加入的一种约束。如果加入其它约束，也会形成更多拓扑结构的网络，比如后续会介绍介于CNN与FNN之间的 __Locally connected neural network__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equivariant to translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络的变种\n",
    "## 在各类不通过的Data Types，卷积神经网络是如何处理的？\n",
    "## 卷积运算中有哪些提高运算效率的方法？\n",
    "## 作为深度学习的先锋，卷积神经网络与神经科学有哪些联系？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
